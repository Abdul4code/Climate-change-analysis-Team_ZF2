{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132677f3",
   "metadata": {},
   "source": [
    "# Climate Change Belief Analysis 2022 - Team ZF2\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "\n",
    "###### Team Members\n",
    "\n",
    "1. Abubakar Abdulkadir\n",
    "2. Joseph Mugo\n",
    "3. Philip Ogunmola\n",
    "4. Rogers Mugambi\n",
    "5. Adewale Nana\n",
    "6. Philip Wambua\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ce7f3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<img src=\"images/climate.jpg\"/>\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, this notebook provides a walthrough the creation of a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eadaea",
   "metadata": {},
   "source": [
    "## Table of Contents     <a id=\"content\"></a>\n",
    "\n",
    "<a href=#1>1. Problem Statement </a>\n",
    "\n",
    "<a href=#1>2. Packages </a>\n",
    "\n",
    "<a href=#2>3. Loading Data</a>\n",
    "\n",
    "<a href=#4>4. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#5>5. Data Cleaning and Engineering</a>\n",
    "\n",
    "<a href=#6>6. Modeling and Evaluation</a>\n",
    "\n",
    "<a href=#7>7. Kaggle Submission </a>\n",
    "\n",
    "<a href=#8>8. Models Deployment </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a696c8e",
   "metadata": {},
   "source": [
    "## 1.0 Problem Statement\n",
    "\n",
    "To build a robust Machine Learning Model that will be able to predict a person’s belief in Climate Change based on their tweet data, allowing companies to gain access into customer sentiment. <br ><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3dc04",
   "metadata": {},
   "source": [
    "## 2.0 Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b40ea",
   "metadata": {},
   "source": [
    "### 2.1. Installing Packages\n",
    "\n",
    "Aside the popular libraries for Machine learning like sklearn, pandas, matplotlib  and numpy, other libraries were used to provide different useful functionality to aid in the development of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install comet_ml \n",
    "!pip install langdetect\n",
    "!pip install imblearn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba8d31",
   "metadata": {},
   "source": [
    "- <a href=\"https://www.comet.ml/\"> comet</a> was used in this project to improve collaboration and control version by means of keeping track of all the stages of our project and using its registry as a source of secure storage. <br><br>\n",
    "\n",
    "- <a href=\"https://pypi.org/project/langdetect/\"> langdetect</a>  was used for detecting words in tweets which does not belong to the language of choice for this project; English. It can recognize about 55 languages. <br><br>\n",
    "\n",
    "- <a href=\"https://imbalanced-learn.org\"> imblearn</a> was used to generate synthetic data for handling the class imbalance in the tweet sentiment. <br><br>\n",
    "\n",
    "- <a href=\"https://www.nltk.org\"> nltk</a> was used for Natural language progressing. It contains methods which allows us to manipulate human language using python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d254bdc",
   "metadata": {},
   "source": [
    "### 2.2 Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71369bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and Text processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Data Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Resampling techniques\n",
    "from collections import Counter \n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# deployment\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a714a",
   "metadata": {},
   "source": [
    "### 2.3 Using Comet \n",
    "\n",
    "* Install Comet\n",
    "* Import Experiment from Comet\n",
    "* Create an experiment instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81290eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an experiment with the comet generated api key:\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"wiR1AvPCwXvVyTJi4os4w9k7h\",\n",
    "#     project_name=\"Climate-change-analysis-Team_ZF2\",\n",
    "#     workspace=\"pheelog\", # I will suggest we use Joe Mugo comet username here\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcb84c",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## 3.0 Loading Data\n",
    "\n",
    "    \n",
    "\n",
    "For this project, Pandas library will be used to access and manipulate the datasets. Hence, The training and testing datasets are loaded from the `train` and `test_with_no_labels` csv files into df_train and df_test variables respectively using the pandas <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\"> read_csv</a> method. The read_csv method returns a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\"> dataframe</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879637dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train dataset\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# read test dataset\n",
    "df_test = pd.read_csv('data/test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f0939",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 4.0 Exploratory Data Analysis\n",
    "\n",
    "\n",
    "Exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. Primarily, EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.This approach for data analysis uses many tools(mainly graphical to maximize insight into a data set, extract important variables, detect outliers and anomalies, amongst other details that is missed when looking at DataFrame. This step is very important especially when we model the data in order to apply Machine Learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec139b",
   "metadata": {},
   "source": [
    "### 4.1 Overview of the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ddc108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd4f98",
   "metadata": {},
   "source": [
    "To have a quick overview of the dataset, the pandas<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html?highlight=head#pandas.DataFrame.head\"> DataFrame.head </a> method was used to display the first five rows of the train dataset. It is immediately obvious that the dataset contains two predictors; message and tweetid with a target variable; sentiment. The message field also contains common tweet characters like # and @ tags.\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93185608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704f50d",
   "metadata": {},
   "source": [
    "A little more exploration with the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html\"> DataFrame.info </a> method reveals that the dataset contains 15,819 entries with no null values. Hence, the shape of the dataset can be deduced to be (15819, 3) <br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceed350",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "* The train datasets contains one categorical column called 'message'\n",
    "* The train dataset contains three columns; one categorical - 'message' and two numerical - 'sentiment and 'tweetid\n",
    "* Some tweets contain twitter handles (e.g @RawStory), numbers (e.g year 2016), hashtags (e.g #TodayinMaker# WIRED) and re-tweets (RT).\n",
    "* Some tweets contain names of ogarnisations, continents and countries.\n",
    "* New lines are represented by '\\n' in the tweet string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0fc2c",
   "metadata": {},
   "source": [
    "### 4.2 Analysis  of the Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104720c0",
   "metadata": {},
   "source": [
    "#### 4.2.1 Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b073b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count occurence of each class in training dataset\n",
    "category_counts = df_train['sentiment'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33e61f",
   "metadata": {},
   "source": [
    "There are four distinct Class labels present in the train data. Hence the the problem is a multi-class classification problem. \n",
    "  \n",
    "        Label   Sentiments    Description\n",
    "         2\t   News:         the tweet link to factual news about climate change\n",
    "         1       Pro:          the tweet supports the belief of man-made climate change\n",
    "         0\t   Neutral:      the tweet neither supports nor refutes the belief of man-made climate change\n",
    "        -1       Anti:         the tweet does not believe in man-made climate change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc385a7",
   "metadata": {},
   "source": [
    "#### 4.2.2 Sentiment Distribution (Visual Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbc4742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "    Calculate the number of tweets per Sentiment \n",
    "    and plot the class distributions results\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "target =df_train['sentiment'].value_counts()\n",
    "sns.barplot(x=['Pro', 'News', 'Neutral', 'Anti'], y=target, ax=ax, palette=\"Blues_d\")\n",
    "plt.title('Class Distributions')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06412836",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "* The sum of the tweets relating to news,neutral and anti is less than half of the total tweets.\n",
    "* The distribution indicates that there is a class imbalance in the train dataset. The effect of this imbalance, if not attended to, is a model which performs very well at categorising samples of a particular category and fails at others. For an instance, a model built on this train data will perform well in categorising tweets which supports man-made climate change (Pro) but performs way lesser at classifying the Anti tweets.\n",
    "* The training dataset is skewed to the Pro sentiment category indicating a strong support for the belief of man-made climate change. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0663b52",
   "metadata": {},
   "source": [
    "### 4.3 Analysis  of the Message feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa0dd3",
   "metadata": {},
   "source": [
    "From the overview of the data carried out prior, there are two features in the dataset; the message and tweetId features. The tweeteId is numerical while the message is object. The tweetid feature contains unique value for all samples. Hence will not be very useful for modelling. It will be beter off being dropped or being converted to the dataset index. This section contains an exploration of the message feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0b573",
   "metadata": {},
   "source": [
    "#### 4.3.1 Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c4b659",
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_bank_set(dataset, word=\"\", category=\"\"):\n",
    "    '''\n",
    "        Creates a list of all the words or characters in the message feature\n",
    "        \n",
    "        Input:\n",
    "            dataset - The dataset to extract words or characters from\n",
    "            category - Filters the dataset by the specified category\n",
    "            type - Specifies the level of extraction; wether characters or words\n",
    "        \n",
    "        Output:\n",
    "            pandas DataFrame of all the characters or words of the specified category \n",
    "    '''\n",
    "    corpus = []\n",
    "    if category:\n",
    "        df = dataset[dataset['sentiment'] == category]['message']\n",
    "    else:\n",
    "        df = dataset['message']\n",
    "    \n",
    "    if word:\n",
    "        bank = []\n",
    "        for row in df:\n",
    "            bank.extend(row.split(\" \"))\n",
    "    else:\n",
    "        bank = [row[x] for row in df for x in range(len(row))]\n",
    "        \n",
    "    return pd.DataFrame(bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f7b58",
   "metadata": {},
   "source": [
    "<br />\n",
    "Using the create_bank_set function to create a word bank for the entire train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = create_bank_set(df_train, word=True)\n",
    "total_words = len(words[0])\n",
    "count_unique = len(words[0].unique())\n",
    "print(\"Total words: {} \\nUnique words: {}\".format(total_words, count_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ed687",
   "metadata": {},
   "source": [
    "There are about 48,000 total words in the unprocessed train dataset. Considering that there are 15,819 total number of tweets, it can be deduced that each tweet will only contain on the average 47,968 / 15,189 = 3.158 unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e4ca7",
   "metadata": {},
   "source": [
    "#### 4.3.2 Words Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of words per Class\n",
    "fig, ax = plt.subplots(2,2, figsize=(15,10))\n",
    "categories = [1, 2, 0, -1]\n",
    "labels = ['Pro', 'News', 'Neutral', 'Anti']\n",
    "\n",
    "pos_x = 0\n",
    "pos_y = 0 \n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    if i > 1:\n",
    "        pos_x = 1\n",
    "        \n",
    "    pos_y = i % 2\n",
    "    \n",
    "    data = create_bank_set(df_train, word=True, category=category)\n",
    "    data = data[0].value_counts().head(10)\n",
    "    sns.barplot(x=data, y=data.index, ax=ax[pos_x][pos_y], palette=\"Blues_d\")\n",
    "    ax[pos_x][pos_y].set_ylabel(\"Count\")\n",
    "    ax[pos_x][pos_y].set_title(\"Most Popular words in \" + str(labels[i]) + \" tweets\")\n",
    "    ax[pos_x][pos_y].set_xlabel(\"Words\")\n",
    "    \n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41889af9",
   "metadata": {},
   "source": [
    "The graphs above showcase the evidence of noise. A lot of stop words are picked up as being important which include: (the, to, and, also of). Also, in the graph labeled popular for news tweets, there is a punctuation (a dash -) picked up as an important word. \n",
    "\n",
    "<a href=\"https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47\"> Stop words </a> are very common words that don’t have a lot of meaning or words that can be safely ignored without sacrificing the meaning of a text like this, that, or etc. From the above distribution, in addition to the obvious stop words, tweets from the different classes appear to contain similar wordings like Climate, change, RT, and global. These words usually will not be considered as stop words but in this case, since they appear the most frequent in almost all the categories, they will behave more like stop words, providing little or no information to our model. Hence, a consideration to drop such common words. <br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397e8ce",
   "metadata": {},
   "source": [
    "#### 4.3.2 Mention Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94904e",
   "metadata": {},
   "source": [
    "A mention is a tweet that contains another person's username anywhere in the body of the tweet. We collect these messages, as well as all the replies including mentions of multiple usernames in a tweet. All those mentioned will see any tweet in which they are mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calculate the number of mentions for each sentiment in a tweet\"\"\"\n",
    "\n",
    "train1 = df_train.copy()\n",
    "# mention count \n",
    "train1['mentions'] = df_train['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n",
    "\n",
    "#plot the number of mentions\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(y='mentions', x='sentiment', data=train1, palette=\"Blues_d\")\n",
    "plt.title('Number of mentions')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(60, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85f039",
   "metadata": {},
   "source": [
    " **Observation:**\n",
    "\n",
    "The Anti and pro setiments seem to have the most mentions per tweet; most of the tweets have atleast one mention as compared to the news and Neutral sentiment classes which have most tweets either having or not having a mention since their avarage mentions per tweet lies between 0.5. Although, expectations would have been that the news class will hold more mentions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3cc1e",
   "metadata": {},
   "source": [
    "#### 4.3.3. Hashtags Analysis\n",
    "\n",
    "People use the hashtag symbol (#) before a relevant keyword or phrase in their Tweet to categorize those Tweets and help them show more easily in Twitter search. Clicking or tapping on a hashtagged word in any message shows you other Tweets that include that hashtag. Hashtags can be included anywhere in a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f220ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to extract hashtags from tweets\n",
    "\n",
    "def extract_hashtags(x):\n",
    "    hashtags = []\n",
    "    for i in x:\n",
    "        ht = re.findall(r'#(\\w+)', i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf481d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hashtags from tweets\n",
    "news_h = extract_hashtags(df_train['message']\n",
    "                              [df_train['sentiment'] == 2])\n",
    "pro_h = extract_hashtags(df_train['message']\n",
    "                          [df_train['sentiment'] == 1])\n",
    "neutral_h = extract_hashtags(df_train['message']\n",
    "                              [df_train['sentiment'] == 0])\n",
    "anti_h = extract_hashtags(df_train['message']\n",
    "                          [df_train['sentiment'] == -1])\n",
    "\n",
    "# hashtag list\n",
    "hashtags = [sum(news_h, []), sum(pro_h, []),\n",
    "            sum(neutral_h, []),sum(anti_h, [])]\n",
    "            \n",
    "# Distribution of words per Class\n",
    "fig, ax = plt.subplots(2,2, figsize=(15,10))\n",
    "categories = [1, 2, 0, -1]\n",
    "labels = ['Pro', 'News', 'Neutral', 'Anti']\n",
    "\n",
    "pos_x = 0\n",
    "pos_y = 0 \n",
    "\n",
    "for i, sent in enumerate(hashtags):\n",
    "    if i > 1:\n",
    "        pos_x = 1\n",
    "        \n",
    "    pos_y = i % 2\n",
    "            \n",
    "    freq_dist = nltk.FreqDist(sent)\n",
    "    df = pd.DataFrame({'Hashtag': list(freq_dist.keys()),\n",
    "                      'Count' : list(freq_dist.values())})\n",
    "\n",
    "    df = df.nlargest(columns='Count', n=15)\n",
    "            \n",
    "    sns.barplot(data=df, y='Hashtag', x='Count', palette=\"Blues_d\", ax=ax[pos_x][pos_y])\n",
    "    ax[pos_x][pos_y].set_ylabel(\"Hastags\")\n",
    "    ax[pos_x][pos_y].set_title(\"Hashtags on the \" + labels[i] + \" sentiment\")\n",
    "    ax[pos_x][pos_y].set_xlabel(\"Frequency\")\n",
    "    \n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751e112",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "* We can see that the top 5 hashtags have similar words like Climate, climate change, Trump and Before the flood\n",
    "* Before the flood is a popular hashtags used in pro climate change tweets, this refers to a 2016 documentary where actor Leonardo DiCaprio meets with scientists, activists and world leaders to discuss the dangers of climate change and possible solutions.\n",
    "* In the anti climate change tweets MAGA (Make America great again) is the top popular hashtag. It is a slogan that was often used by Donald Trump during his campaign for elections in 2016. This soon became a trending hashtag to use to show support for Donald Trump., "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77742d37",
   "metadata": {},
   "source": [
    "#### 4.3.4. Retweets Analysis\n",
    "\n",
    "Another very popular words in all the classes is the RT which stands for retweet. Twitter allows a user to retweet, or RT another users tweets. This is great for creating trends, but not useful for sentiment analysis. Now we will remove the duplicates to get a clearer picture of our data set. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution for set of retweeted-tweets and set without retweets\n",
    "plt.figure(figsize = (10,7))\n",
    "train1['retweet'] = train1['message'].apply(lambda tweet: 1 if tweet.startswith('RT @') else 0)\n",
    "sns.countplot(x='retweet', data=train1, palette='Blues_d', hue='sentiment')\n",
    "plt.title('Number of Retweets Per Sentiment Class',fontsize=14)\n",
    "plt.xlabel('Retweet')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838fb4b",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* The pro sentiment has about 6000 retweets as compared to the about its 3000 tweets. The retweets are twice the number of tweets\n",
    "\n",
    "* Other class have the retweet almost equal to the number of actual tweets\n",
    "\n",
    "* This indicates a relative higher support ratio for the pro sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426124b1",
   "metadata": {},
   "source": [
    "#### 4.3.5. URL Analysis\n",
    "\n",
    "There are also URL related words occuring frequently as indicated by the distribution of words. This section seeks to explore if links are more peculiar to some classes than others.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae56c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting urls\n",
    "train1['urls'] = train1['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n",
    "\n",
    "# ploting the number of urls\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(y='urls', x='sentiment', data=train1, palette=\"Blues_d\")\n",
    "plt.title('Number of urls')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches( 30, 7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4b0a5",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "There is not much difference between the number of urls in each setiment. Although, as expected, most tweets in the pro sentiment contain atleast a link while the other classes either have or do not. Generally, links can not be attributed to a particular sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29819b1a",
   "metadata": {},
   "source": [
    "#### 4.3.5. Twitter Handles Analysis\n",
    "\n",
    "A Twitter handle is the username that appears at the end of your unique Twitter URL. Twitter handles appear after the @ sign in your profile URL and it must be unique to your account. A Twitter name, on the other hand, is simply there to help people find the company they're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09120b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to extract handles from tweets\n",
    "def extract_handles(x):\n",
    "    handles = []\n",
    "    for i in x:\n",
    "        h = re.findall(r'@(\\w+)', i)\n",
    "        handles.append(h)\n",
    "        \n",
    "    return handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting handles from tweets\n",
    "news_h = extract_handles(df_train['message']\n",
    "                              [df_train['sentiment'] == 2])\n",
    "pro_h = extract_handles(df_train['message']\n",
    "                          [df_train['sentiment'] == 1])\n",
    "neutral_h = extract_handles(df_train['message']\n",
    "                              [df_train['sentiment'] == 0])\n",
    "anti_h = extract_handles(df_train['message']\n",
    "                          [df_train['sentiment'] == -1])\n",
    "\n",
    "# handle lists \n",
    "handles = [sum(news_h, []), sum(pro_h, []), sum(neutral_h, []),\n",
    "           sum(anti_h, [])]\n",
    "\n",
    "# Distribution of words per Class\n",
    "fig, ax = plt.subplots(2,2, figsize=(15,10))\n",
    "categories = [1, 2, 0, -1]\n",
    "labels = ['Pro', 'News', 'Neutral', 'Anti']\n",
    "\n",
    "pos_x = 0\n",
    "pos_y = 0 \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [50, 5]\n",
    "\n",
    "for i, sent in enumerate(handles):\n",
    "    if i > 1:\n",
    "        pos_x = 1\n",
    "        \n",
    "    pos_y = i % 2\n",
    "   \n",
    "    freq_dist = nltk.FreqDist(sent)\n",
    "    df = pd.DataFrame({'Handle': list(freq_dist.keys()),\n",
    "                      'Count' : list(freq_dist.values())})\n",
    "\n",
    "    df = df.nlargest(columns='Count', n=15)\n",
    "\n",
    "    sns.barplot(data=df, y='Handle', x='Count', palette='Blues_d', ax=ax[pos_x][pos_y])\n",
    "    ax[pos_x][pos_y].set_ylabel(\"Handles\")\n",
    "    ax[pos_x][pos_y].set_title(\"Twitter Handles on the \" + labels[i] + \" sentiment\")\n",
    "    ax[pos_x][pos_y].set_xlabel(\"Frequency\")\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffc710",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "From the Visuals above we can all that ...\n",
    "* The most popular News handles are actual news broadcaster accounts\n",
    "* The most popular Pro handles seem to be celebrity accounts & news accounts.\n",
    "* Trump features most for most popular Anti & Neutral tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc34d98",
   "metadata": {},
   "source": [
    "## 5.0 Data Cleaning and Engineering\n",
    "\n",
    "\n",
    "In this section, the recommendations from the exploratory data analysis phase is implemented. The dataset was cleaned and features engineered from the message field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ae5cf",
   "metadata": {},
   "source": [
    "### 5.1 Text Cleaning\n",
    "\n",
    "The dataset contains punctuations, links, emojis and twitter specific characters like @ and # symbols. Words also exist in different cases which models might translate and different. Hence, the proceeding function performs cleaning by:\n",
    "- Changing the Case of the words\n",
    "- Remove punctuations\n",
    "- Remove links\n",
    "- Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca894ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    # change the case of all the words in the text to lowercase \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove links from the text\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text =  url.sub(r'', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = \"\".join([x for x in text if x not in string.punctuation])\n",
    "    \n",
    "    # Remove Emojis - Emoji Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    '''# Correct mispelt word\n",
    "    spelling = SpellChecker()\n",
    "    correct_text = []\n",
    "    misspelt_words = spelling.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelt_words:\n",
    "            correct_text.append(spelling.correction(word))\n",
    "        else:\n",
    "            correct_text.append(word)\n",
    "    text = \" \".join(correct_text)'''\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bd0c1",
   "metadata": {},
   "source": [
    "The clean_data function accepts a row of text and returns a string with the punctuations, emojis and links removed including changing the case and correcting misspelt words. <br><br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99d80be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt rawstory researchers say we have three year...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221\n",
       "1          1  its not like we lack evidence of anthropogenic...   126103\n",
       "2          2  rt rawstory researchers say we have three year...   698562\n",
       "3          1  todayinmaker wired  2016 was a pivotal year in...   573736\n",
       "4          1  rt soynoviodetodas its 2016 and a racist sexis...   466954"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the train dataset\n",
    "df_train['message'] = df_train['message'].apply(clean_data)\n",
    "\n",
    "# Clean the test dataset\n",
    "df_test['message'] = df_test['message'].apply(clean_data)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b57bd0",
   "metadata": {},
   "source": [
    "### 5.2 Drop TweetId feature\n",
    "\n",
    "The tweetId feature contains unique values across the feature. Hence, it will contribute little or nothing to the accuracy of the model. While weighing down the model in terms of computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbf2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unwanted(dataframe, unwanted_features):\n",
    "    df = dataframe.drop(unwanted_features,  axis='columns')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e7a2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt rawstory researchers say we have three year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...\n",
       "1          1  its not like we lack evidence of anthropogenic...\n",
       "2          2  rt rawstory researchers say we have three year...\n",
       "3          1  todayinmaker wired  2016 was a pivotal year in...\n",
       "4          1  rt soynoviodetodas its 2016 and a racist sexis..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop tweetid from train dataset\n",
    "df_train = drop_unwanted(df_train, ['tweetid'])\n",
    "\n",
    "# Reserve tweetid for prediction\n",
    "test_tweet_id = df_test['tweetid']\n",
    "\n",
    "# drop tweetid from test dataset\n",
    "df_test = drop_unwanted(df_test, ['tweetid'])\n",
    "\n",
    "# view snapshot\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b123b",
   "metadata": {},
   "source": [
    "### 5.3 Text Tokenization\n",
    "\n",
    "In this section, the message feature was tokenized (each message was broken down into list of words) using the nltk RegexTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175a3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454b58a",
   "metadata": {},
   "source": [
    "The function above takes a row of text and return a list of each word in the text. <br /><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76bd6f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [polyscimajor, epa, chief, doesnt, think, carb...\n",
       "1    [its, not, like, we, lack, evidence, of, anthr...\n",
       "2    [rt, rawstory, researchers, say, we, have, thr...\n",
       "3    [todayinmaker, wired, 2016, was, a, pivotal, y...\n",
       "4    [rt, soynoviodetodas, its, 2016, and, a, racis...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the train dataset\n",
    "df_train['message'] = df_train['message'].apply(tokenize_data)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "df_test['message'] = df_test['message'].apply(tokenize_data)\n",
    "\n",
    "df_train['message'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddd13e",
   "metadata": {},
   "source": [
    "### 5.4 Stop words Removal\n",
    "\n",
    "Stop words from the message feature are removed using the defined function remove_stop_words. <br ><br >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(list_of_words):\n",
    "    # The function takes a list of words and filter out the stop words\n",
    "    words = [word for word in list_of_words if word not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from train dataset\n",
    "df_train['message'] = df_train['message'].apply(remove_stop_words)\n",
    "\n",
    "# Remove stop words from test dataset\n",
    "df_test['message'] = df_test['message'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a6948",
   "metadata": {},
   "source": [
    "### 5.5 Lemmatization \n",
    "\n",
    "English words have different variants. cats and cat are variants of cat. Lemmatization will ensure that all variants of a word map to the same root word. ALthough, an alternative to lemmatization is stemming. Stemming will remove the suffix of words and does not necessarily return English words. Hence, Lematization is preffered in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e76d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(list_of_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(x) for x in list_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a194ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize train set\n",
    "df_train['message'] = df_train['message'].apply(lemmatize_words)\n",
    "\n",
    "# lemmatize test set\n",
    "df_test['message'] = df_test['message'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c0687",
   "metadata": {},
   "source": [
    "After the lemmatization, we join the tokenized words together. This is to enable us create features from the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790ec768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all the words in the message field\n",
    "df_train['message'] = df_train['message'].apply(lambda x: \" \".join(x))\n",
    "df_test['message'] = df_test['message'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c1587",
   "metadata": {},
   "source": [
    "### 5.6 Features Extraction \n",
    "\n",
    "To create a model, there is a need to have a set of feature(s) and target. Most models only accept numerical values for feature sets. For this project, our feature is a string of words. Hence there is a need to create vectors of digits from these words. The process is called Vectorization.\n",
    "\n",
    "For this project we define a vectorizer with the following tuning\n",
    "- stop_words = 'english'\n",
    "- ngram_range = (1, 2)\n",
    "- max_df = 0.5\n",
    "- min_df = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0bef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(train, test):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                                 min_df=1,\n",
    "                                 ngram_range=(1, 2),\n",
    "                                 max_df = 0.5)\n",
    "    train_data = vectorizer.fit_transform(train)\n",
    "    test_data = vectorizer.transform(test)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                                 min_df=1,\n",
    "                                 ngram_range=(1, 2),\n",
    "                                 max_df = 0.5)\n",
    "new_data = vectorizer.fit_transform(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06ca7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the train and test dataset\n",
    "train_data, test_data = vectorize_data(df_train['message'], df_test['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dca137",
   "metadata": {},
   "source": [
    "### 5.7 Spliting Train Dataset \n",
    "\n",
    "To properly assess a model, we need to assess the model on data which it has not seen before. Hence, there is a need to split the training dataset into two different datasets; the training dataset - which will be used to train a model and the validation set - which will be used to measure the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c1bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, validation_x, train_y, validation_y = train_test_split(train_data, df_train['sentiment'], test_size=0.2,  random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44cdf404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12655, 110327)\n",
      "(3164, 110327)\n",
      "(12655,)\n",
      "(3164,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(validation_x.shape)\n",
    "print(train_y.shape)\n",
    "print(validation_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041b01f",
   "metadata": {},
   "source": [
    "### 5.8 Feature Selection\n",
    "\n",
    "The test and train dataset contains a large number of features, 105,665 features precisely. Performance wise, not all of these features will make a positive performance difference on the model. Although, they can have negative impact in terms of computational cost and performance.  It is therefore imperative to select features which contirbutes positively to the result of the model. For these project, the KBest method of features selection was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d519da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the feature selector module\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Set up selector, choosing score function and number of features to retain\n",
    "selector_kbest = feature_selection.SelectKBest(score_func=f_classif, k=95000)\n",
    "\n",
    "# Transform (i.e.: run selection on) the training data\n",
    "train_x_new = selector_kbest.fit_transform(train_x, train_y)\n",
    "valid_x_new = selector_kbest.transform(validation_x)\n",
    "all_train_new = selector_kbest.transform(train_data)\n",
    "all_test_new = selector_kbest.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2fd30",
   "metadata": {},
   "source": [
    "### 5.9 Balancing the classes with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30769b2",
   "metadata": {},
   "source": [
    "Class imbalance is a scenario where the training dataset is skewed towards some classes. That it, there is more samples of data in some classes than others. So the models gets really good at predicting some classes and fails or perfroms poorly at others. Intuitively, class imbalance can cause a threat to the performance of the model. Hence, the classes in the train dataset is balanced using the SMOTE (Synthetic Minority Oversampling Technique). This technique will generate synthetic or artificial data to upsample classes with low distribution of samples. This technique is favoured because it reduces the chances of loss of data and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=20)\n",
    "train_x_new, train_y_new = sm.fit_resample(train_x_new, train_y)\n",
    "all_train_new, all_y_new = sm.fit_resample(all_train_new, df_train['sentiment'])\n",
    "\n",
    "print(\"The training dataset has \" + str(train_x_new.shape) + \" shape\")\n",
    "print(\"The testing dataset has \" + str(train_x_new.shape) + \" shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4a53b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 6.0 Modeling and Evaluation\n",
    "\n",
    "This section of the project focuses on the creation, training and evaluation of classification models. The created models are evaluated on accuracy, precision and recall metrics to select the best for the target purpose.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05814e49",
   "metadata": {},
   "source": [
    "### 6.1 Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059e075",
   "metadata": {},
   "source": [
    "The <a href=\"https://www.upgrad.com/blog/random-forest-classifier/\">Random Forest Classifier</a> is among the most popular classification algorithms. It is a supervised learning algorithm which can be used for regression and classification problems. The choice of trying this model includes its significance in terms of accuracy than most of the non-linear classifiers. It is also very robust because it uses multiple decision trees to arrive at its result. The minimum accuracy benchmark for this project is 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629ab31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       0.94      0.27      0.42       272\n",
      "      1: Pro       0.66      0.27      0.39       477\n",
      "  0: Neutral       0.68      0.94      0.79      1726\n",
      "    -1: Anti       0.82      0.62      0.70       689\n",
      "\n",
      "    accuracy                           0.71      3164\n",
      "   macro avg       0.77      0.52      0.57      3164\n",
      "weighted avg       0.73      0.71      0.68      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_model = RandomForestClassifier(random_state=10)\n",
    "tree_model.fit(train_x, train_y)\n",
    "predictions = tree_model.predict(validation_x)\n",
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad258480",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "The Random Forest Classifier model, from the confusion matrix, indicates that the model will generate the correct prediction 71% of the time (accuracy) - just a little above the base accuracy (70%). For individual classes, the model will be able to clasify tweets correctly above 66% of the time for all classes (TP), the other 34% of the time, it will mistake other classes(FP). Although it will be able to detect only above 27% of the total tweets for two classes. Hence, there is a need to try other models in a bid to find better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fc224",
   "metadata": {},
   "source": [
    "### 6.2 Ridge Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce3c3c6",
   "metadata": {},
   "source": [
    "Unlike the Random Forest Classifier, the Ridge Classifier is a linear model which builds on the Linear regressor to improve performance. Since the considered problem is high dimentional (many features) and high dimensional problems are likely to be linearly separable meaning the different point can be sepehrated with a linear classifier, regardless of how the points are labelled. So linear classifiers like ridge regression or SVM with a linear kernel, are likely to do well. Hence, the choice to implement the Ridge Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffbab370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = RidgeClassifier()\n",
    "ridge_model.fit(train_x, train_y)\n",
    "predictions = ridge_model.predict(validation_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ef62c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       0.91      0.33      0.49       272\n",
      "      1: Pro       0.71      0.30      0.42       477\n",
      "  0: Neutral       0.71      0.93      0.80      1726\n",
      "    -1: Anti       0.80      0.68      0.74       689\n",
      "\n",
      "    accuracy                           0.73      3164\n",
      "   macro avg       0.78      0.56      0.61      3164\n",
      "weighted avg       0.75      0.73      0.71      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab033b",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "The Ridge Classifier model, from the confusion matrix, indicates that the model will generate the correct prediction 73% of the time (accuracy). It is an improvement to the Random Forest Classifier. For individual class, the model will identify identify tweet correctly (precision) above 70% for all the classes. It will also be able to detect above 60% of the total positive class (recall) for two of the classes and above 30% for the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00635e98",
   "metadata": {},
   "source": [
    "### 6.3 Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ac132",
   "metadata": {},
   "source": [
    "This classifier was experimented for similar reason with the Ridge Classifier. It is worthy of being experimented on because the linear Ridge Classifiier performs better than the unlinear Random Forest Classifier. Hence, the experimentation on Support Vector Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a81ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       0.95      0.30      0.46       272\n",
      "      1: Pro       0.71      0.31      0.43       477\n",
      "  0: Neutral       0.70      0.94      0.80      1726\n",
      "    -1: Anti       0.83      0.66      0.74       689\n",
      "\n",
      "    accuracy                           0.73      3164\n",
      "   macro avg       0.80      0.55      0.61      3164\n",
      "weighted avg       0.75      0.73      0.70      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_model = SVC(gamma=2, C=1, kernel='linear')\n",
    "svc_model.fit(train_x, train_y)\n",
    "predictions = svc_model.predict(validation_x)\n",
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38be726",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "As expected, the performance of the Support Vector Classifier is similar to the Ridge Classifier modelwith an accuracy of 73%. It can be use alternatively to the Ridge Regressor for similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f2ab9",
   "metadata": {},
   "source": [
    "### 6.4 Nearest Neighbour Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a064c",
   "metadata": {},
   "source": [
    "Having considered models based on linear and unlinear layout of datapoints, an algorithms which based on an entirely different appraoch is considered. Hence, the K-Nearest Neighbor classifier is considered. The K-Nearest Neighbor classifier is a nonparametric classification method which uses plurality of vote of its neighbors for classification. K is the defined number of neighbors used in voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4dd55fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       0.57      0.28      0.37       272\n",
      "      1: Pro       0.38      0.49      0.43       477\n",
      "  0: Neutral       0.70      0.80      0.75      1726\n",
      "    -1: Anti       0.72      0.46      0.56       689\n",
      "\n",
      "    accuracy                           0.64      3164\n",
      "   macro avg       0.59      0.51      0.53      3164\n",
      "weighted avg       0.65      0.64      0.63      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(10)\n",
    "knn_model.fit(train_x, train_y)\n",
    "predictions = knn_model.predict(validation_x)\n",
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e77a7",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "The KNearest Neighbour performs worst than the previous experimented models. Its accuracy is way below the 70% benchmark. This model, if deployed will fail to identity the Pro tweets, it has only 38% precision rate for the class insipte of its recall being 49%. On the News tweets, its performance is no better than randomly guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58705b7e",
   "metadata": {},
   "source": [
    "### 6.5 AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20523614",
   "metadata": {},
   "source": [
    "Adaptive Boosting (AdaBoost) Classifier combines several simple and week models and learners into strong models for classification purpose. This model is experimented in a bid to improve on the performances of the other simpler models tried previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3fb5062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       0.64      0.30      0.41       272\n",
      "      1: Pro       0.86      0.03      0.05       477\n",
      "  0: Neutral       0.61      0.91      0.73      1726\n",
      "    -1: Anti       0.62      0.40      0.49       689\n",
      "\n",
      "    accuracy                           0.61      3164\n",
      "   macro avg       0.68      0.41      0.42      3164\n",
      "weighted avg       0.65      0.61      0.55      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_model = AdaBoostClassifier()\n",
    "ada_model.fit(train_x, train_y)\n",
    "predictions = ada_model.predict(validation_x)\n",
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fa6e7",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "The Performance of the AdaBoost classifier is similar to the KNN classifier in terms of its general accuracy. ALthough, for identification of idividual classes correctly (TP), it has a better performance - abobe 60% precision for all the classes. Although, its True positive rate is questionable as it identifies only small portion of the total True positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7405b55",
   "metadata": {},
   "source": [
    "### 6.6 Stochastic Gradient Descent  Classifier (SGD-Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b1289",
   "metadata": {},
   "source": [
    "SGD Classifier is a linear classifier (SVM, logistic regression, a.o.) optimized by the SGD. These are two different concepts. While SGD is a optimization method, Logistic Regression or linear Support Vector Machine is a machine learning algorithm/model. You can think of that as while a machine learning model defines a loss function the optimization method minimizes/maximizes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "275aaefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       0.92      0.34      0.49       272\n",
      "      1: Pro       0.75      0.29      0.42       477\n",
      "  0: Neutral       0.71      0.94      0.81      1726\n",
      "    -1: Anti       0.79      0.69      0.74       689\n",
      "\n",
      "    accuracy                           0.73      3164\n",
      "   macro avg       0.79      0.56      0.61      3164\n",
      "weighted avg       0.75      0.73      0.71      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgdc_model = SGDClassifier()\n",
    "sgdc_model.fit(train_x, train_y)\n",
    "predictions = sgdc_model.predict(validation_x)\n",
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93998774",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "This model performs slighly better than the ridge regressor. Although, both model are similar in precision and recall, the F1 score indicates a little increase in performance for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5a3f6",
   "metadata": {},
   "source": [
    "### 6.7 Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57232ffd",
   "metadata": {},
   "source": [
    "Multinomial Naïve Bayes uses the frequency of words in the feature set for classification. This classification model has a track record of performing well in text classification on small sample sizes. It is experimented here due to its popularity and recommendations due to its performance. It is also very computationally effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04f36259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     2: News       1.00      0.04      0.07       272\n",
      "      1: Pro       1.00      0.06      0.12       477\n",
      "  0: Neutral       0.61      0.99      0.76      1726\n",
      "    -1: Anti       0.91      0.43      0.58       689\n",
      "\n",
      "    accuracy                           0.65      3164\n",
      "   macro avg       0.88      0.38      0.38      3164\n",
      "weighted avg       0.77      0.65      0.56      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bayes_model = MultinomialNB()\n",
    "bayes_model.fit(train_x, train_y)\n",
    "predictions = bayes_model.predict(validation_x)\n",
    "print(classification_report(validation_y, predictions, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95c77e",
   "metadata": {},
   "source": [
    "This model boost of a 100% precision for two classes and above 60% for the others. Using this metric alone  might confuse one into believing in the performance of this model. Although, a look at the recall, we can see that the model can only identify 0.4% which is less than 1 of the total positives for the News and Pro tweets. This model would have been useful if the goal of this project is to identify neutral or Anti tweets only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e248e",
   "metadata": {},
   "source": [
    "## 7.0 Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39648f2e",
   "metadata": {},
   "source": [
    "The kaggle submission is accessed based on accuracy of the model. Therefore for this task, the selection of model will be based on classification accuracy. From the modeling and evaluation section, the ridge model has the highest accuracy score. Hence, a ridge regression model is trained and used for prediction to be submitted to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37078787",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ridge_model = RidgeClassifier()\n",
    "final_ridge_model.fit(train_data, df_train['sentiment'])\n",
    "predictions = final_ridge_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe7d3de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10546,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'tweetid': test_tweet_id, 'sentiment':predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f11ad",
   "metadata": {},
   "source": [
    "## 8.0 Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cded30",
   "metadata": {},
   "source": [
    "The models developed in this notebook is intended to be deployed on the opiniate web application. Since each model has its strenght and weeknesses, the application will contain section which allows users to select which model they intend to use. Hence, a copy of all the useful models trained in this notebook is pickeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53ad0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle random forest\n",
    "with open('models/random_forest.pkl', 'wb') as fh:\n",
    "   pickle.dump(tree_model, fh)\n",
    "\n",
    "# pickle ridge Classifier\n",
    "with open('models/ridge.pkl', 'wb') as fh:\n",
    "   pickle.dump(ridge_model, fh)\n",
    "\n",
    "# pickle support vector classifier \n",
    "with open('models/svc.pkl', 'wb') as fh:\n",
    "   pickle.dump(svc_model, fh)\n",
    "\n",
    "# pickle Knn classifier \n",
    "with open('models/knn.pkl', 'wb') as fh:\n",
    "   pickle.dump(knn_model, fh)\n",
    "\n",
    "# pickle adaboost classifier \n",
    "with open('models/ada.pkl', 'wb') as fh:\n",
    "   pickle.dump(ada_model, fh)\n",
    "\n",
    "# pickle bayes classifier \n",
    "with open('models/bayes.pkl', 'wb') as fh:\n",
    "   pickle.dump(bayes_model, fh)\n",
    "\n",
    "# pickle sgdc classifier \n",
    "with open('models/sgdc.pkl', 'wb') as fh:\n",
    "   pickle.dump(sgdc_model, fh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
